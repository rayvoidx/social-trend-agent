"""
ë‰´ìŠ¤ íŠ¸ë Œë“œ ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ LangGraph ì •ì˜

LangGraph ê³µì‹ íŒ¨í„´ê³¼ ì—ëŸ¬ í•¸ë“¤ë§, ë¡œê¹… ê¸°ëŠ¥ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
"""
import os
import uuid
import logging
from typing import Dict, Any
from langgraph.graph import StateGraph, END

from src.core.state import NewsAgentState
from src.core.logging import AgentLogger
from src.core.errors import PartialResult, CompletionStatus, safe_api_call
from src.agents.news_trend.tools import (
    search_news,
    analyze_sentiment,
    extract_keywords,
    summarize_trend,
    retrieve_relevant_items,
    redact_pii,
    check_safety,
)

# Initialize module-level logger (without run_id for module-level logging)
_module_logger = logging.getLogger("news_trend_agent")


def collect_node(state: NewsAgentState) -> Dict[str, Any]:
    """
    ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘

    ì—ëŸ¬ í•¸ë“¤ë§ì„ í†µí•´ API ì‹¤íŒ¨ë¥¼ ìš°ì•„í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("collect")
    logger.info(f"Collecting news: query={state.query}, time_window={state.time_window}")

    # Use safe_api_call for error handling
    result = PartialResult(status=CompletionStatus.FULL)

    raw_items = safe_api_call(
        "search_news",
        search_news,
        query=state.query,
        time_window=state.time_window or "7d",
        language=state.language,
        max_results=state.max_results,
        fallback_value=[],
        result_container=result
    )

    logger.node_end("collect", output_size=len(raw_items))

    return {
        "raw_items": raw_items,
        "error": result.errors[0] if result.errors else None
    }


def normalize_node(state: NewsAgentState) -> Dict[str, Any]:
    """
    ìˆ˜ì§‘ëœ ë°ì´í„° ì •ê·œí™” ë° ì •ì œ

    ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œë¥¼ ìœ„í•œ ì¼ê´€ëœ ë°ì´í„° êµ¬ì¡°ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("normalize", input_size=len(state.raw_items))

    normalized = []
    for item in state.raw_items:
        # Clean HTML tags and normalize fields
        normalized.append({
            "title": item.get("title", "").strip(),
            "description": item.get("description", "").strip(),
            "url": item.get("url", ""),
            "source": item.get("source", {}).get("name", "Unknown") if isinstance(item.get("source"), dict) else str(item.get("source", "Unknown")),
            "published_at": item.get("publishedAt", ""),
            "content": item.get("content", "").strip()
        })

    logger.node_end("normalize", output_size=len(normalized))

    return {"normalized": normalized}


def analyze_node(state: NewsAgentState) -> Dict[str, Any]:
    """
    ê°ì„± ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ

    ê°ì„± ë¶„ì„ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê°œë…ì ìœ¼ë¡œ ë³‘ë ¬ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("analyze", input_size=len(state.normalized))

    # Analyze sentiment with error handling
    result_sentiment = PartialResult(status=CompletionStatus.FULL)
    sentiment_results = safe_api_call(
        "analyze_sentiment",
        analyze_sentiment,
        items=state.normalized,
        fallback_value={"positive": 0, "neutral": 0, "negative": 0},
        result_container=result_sentiment
    )

    # Extract keywords with error handling
    result_keywords = PartialResult(status=CompletionStatus.FULL)
    keyword_results = safe_api_call(
        "extract_keywords",
        extract_keywords,
        items=state.normalized,
        fallback_value={"top_keywords": [], "total_unique_keywords": 0},
        result_container=result_keywords
    )

    analysis = {
        "sentiment": sentiment_results,
        "keywords": keyword_results,
        "total_items": len(state.normalized)
    }

    logger.node_end("analyze", output_size=len(state.normalized))

    return {"analysis": analysis}


def summarize_node(state: NewsAgentState) -> Dict[str, Any]:
    """
    LLMì„ í™œìš©í•œ íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸ ìš”ì•½

    ê²¬ê³ í•œ LLM í˜¸ì¶œì„ ìœ„í•´ LangChainê³¼ ì¬ì‹œë„ ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("summarize")

    # Use LLM to summarize trend with error handling
    result = PartialResult(status=CompletionStatus.FULL)
    # Retrieve relevant subset (RAG)
    relevant = retrieve_relevant_items(state.query, state.normalized, min(10, len(state.normalized)))

    raw_summary = safe_api_call(
        "summarize_trend",
        summarize_trend,
        query=state.query,
        normalized_items=relevant,
        analysis=state.analysis,
        fallback_value="íŠ¸ë Œë“œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ì„œë¹„ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        result_container=result
    )

    # Guardrails
    pii = redact_pii(raw_summary)
    safety = check_safety(pii["redacted"]) if isinstance(pii, dict) else {"unsafe": False, "categories": []}
    summary = pii["redacted"] if isinstance(pii, dict) else raw_summary

    logger.node_end("summarize", output_size=len(summary))

    return {"analysis": {**state.analysis, "summary": summary, "safety": safety}}


def report_node(state: NewsAgentState) -> Dict[str, Any]:
    """
    ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±

    ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("report")

    # Build markdown report
    report_lines = [
        f"# ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸",
        f"",
        f"**ê²€ìƒ‰ì–´**: {state.query}",
        f"**ê¸°ê°„**: {state.time_window or '7d'}",
        f"**ì–¸ì–´**: {state.language}",
        f"**ë¶„ì„ í•­ëª© ìˆ˜**: {len(state.normalized)}",
        f"",
        f"---",
        f"",
        f"## ğŸ“Š ê°ì„± ë¶„ì„",
        f"",
    ]

    sentiment = state.analysis.get("sentiment", {})
    report_lines.extend([
        f"- ê¸ì •: {sentiment.get('positive', 0)}ê°œ ({sentiment.get('positive_pct', 0):.1f}%)",
        f"- ì¤‘ë¦½: {sentiment.get('neutral', 0)}ê°œ ({sentiment.get('neutral_pct', 0):.1f}%)",
        f"- ë¶€ì •: {sentiment.get('negative', 0)}ê°œ ({sentiment.get('negative_pct', 0):.1f}%)",
        f"",
        f"---",
        f"",
        f"## ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ",
        f"",
    ])

    keywords = state.analysis.get("keywords", {}).get("top_keywords", [])
    for kw in keywords[:10]:
        report_lines.append(f"- **{kw['keyword']}** ({kw['count']}íšŒ)")

    report_lines.extend([
        f"",
        f"---",
        f"",
        f"## ğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸",
        f"",
        state.analysis.get("summary", "No summary available."),
        f"",
    ])

    analysis = state.analysis if isinstance(state.analysis, dict) else {}
    safety = analysis.get("safety", {})
    if safety:  # dictê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´
        if safety.get("pii_found") or safety.get("unsafe"):
            report_lines.extend([
                f"---",
                f"",
                f"## ğŸ”’ ì•ˆì „ ë° í”„ë¼ì´ë²„ì‹œ",
                f"- ì¼ë¶€ PII ì •ë³´ê°€ ë§ˆìŠ¤í‚¹ë˜ì—ˆìŠµë‹ˆë‹¤." if safety.get("pii_found") else "",
                f"- ì•ˆì „ ì¹´í…Œê³ ë¦¬ ê°ì§€: {', '.join(safety.get('categories', []))}" if safety.get("unsafe") else "",
                f"",
            ])

    report_lines.extend([
        f"---",
        f"",
        f"## ğŸ“° ì£¼ìš” ë‰´ìŠ¤ (Top 5)",
        f"",
    ])

    for i, item in enumerate(state.normalized[:5], 1):
        report_lines.extend([
            f"### {i}. {item['title']}",
            f"**ì¶œì²˜**: [{item['source']}]({item['url']})",
            f"**ë°œí–‰ì¼**: {item['published_at']}",
            f"",
            f"{item['description']}",
            f"",
        ])

    report_lines.extend([
        f"---",
        f"",
        f"**âš ï¸ ì£¼ì˜**: ë³¸ ë¦¬í¬íŠ¸ëŠ” AIê°€ ìƒì„±í•œ ë¶„ì„ìœ¼ë¡œ, ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        f"ì¶œì²˜ ë§í¬ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.",
        f"",
        f"**Run ID**: `{state.run_id}`",
        f""
    ])

    report_md = "\n".join(report_lines)

    # Calculate metrics
    metrics = {
        "coverage": len(state.normalized) / max(state.max_results, 1),
        "factuality": 1.0 if all(item.get("url") for item in state.normalized) else 0.0,
        "actionability": 1.0 if state.analysis.get("summary") else 0.0
    }

    logger.node_end("report", output_size=len(report_md))

    return {"report_md": report_md, "metrics": metrics}


def plan_node(state: NewsAgentState) -> Dict[str, Any]:
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("plan")
    plan = [
        "Collect",
        "Normalize",
        "Analyze",
        "RAG",
        "Summarize+Guard",
        "Report+Notify",
    ]
    logger.node_end("plan")
    return {"plan": plan}


def critic_node(state: NewsAgentState) -> Dict[str, Any]:
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("critic")
    analysis = state.analysis or {}
    review = {
        "has_sentiment": "sentiment" in analysis,
        "has_keywords": "keywords" in analysis,
        "has_summary": bool(analysis.get("summary")),
    }
    logger.node_end("critic")
    return {"review": review}


def notify_node(state: NewsAgentState) -> Dict[str, Any]:
    """
    ì•Œë¦¼ ì „ì†¡ (n8n, Slack ë“±)

    ì„¤ì •ëœ ì›¹í›… ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    logger = AgentLogger("news_trend_agent", state.run_id)
    logger.node_start("notify")

    notifications_sent = []

    # n8n webhook
    n8n_webhook = os.getenv("N8N_WEBHOOK_URL")
    if n8n_webhook:
        try:
            import requests
            payload = {
                "query": state.query,
                "metrics": state.metrics,
                "run_id": state.run_id,
                "summary": state.analysis.get("summary", "")[:500]  # First 500 chars
            }
            response = requests.post(n8n_webhook, json=payload, timeout=10)
            if response.status_code == 200:
                notifications_sent.append("n8n")
                logger.info("n8n notification sent successfully")
        except Exception as e:
            logger.warning("Failed to send n8n notification", error=str(e))

    # Slack webhook
    slack_webhook = os.getenv("SLACK_WEBHOOK_URL")
    if slack_webhook:
        try:
            import requests
            payload = {
                "text": f"ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ: {state.query}",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"*ê²€ìƒ‰ì–´*: {state.query}\n*ë¶„ì„ í•­ëª©*: {len(state.normalized)}ê±´"
                        }
                    }
                ]
            }
            response = requests.post(slack_webhook, json=payload, timeout=10)
            if response.status_code == 200:
                notifications_sent.append("slack")
                logger.info("Slack notification sent successfully")
        except Exception as e:
            logger.warning("Failed to send Slack notification", error=str(e))

    logger.node_end("notify", output_size=len(notifications_sent))

    return {}


def build_graph():
    """
    ë‰´ìŠ¤ íŠ¸ë Œë“œ ì—ì´ì „íŠ¸ìš© LangGraph êµ¬ì¶•

    LangGraph ê³µì‹ íŒ¨í„´ì„ ë”°ë¦…ë‹ˆë‹¤:
    - Pydantic ìƒíƒœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” StateGraph
    - ì—ëŸ¬ í•¸ë“¤ë§ì„ í¬í•¨í•œ ìˆœì°¨ì  íŒŒì´í”„ë¼ì¸
    - ì—ëŸ¬ ë³µêµ¬ë¥¼ ìœ„í•œ ì¡°ê±´ë¶€ ì—£ì§€ (í–¥í›„ ê°œì„  ì˜ˆì •)

    Returns:
        ì‹¤í–‰ ì¤€ë¹„ê°€ ì™„ë£Œëœ ì»´íŒŒì¼ëœ StateGraph
    """
    _module_logger.info("Building LangGraph for News Trend Agent")

    # Create StateGraph with NewsAgentState (official pattern)
    graph = StateGraph(NewsAgentState)

    # Add nodes (official pattern: node_name, node_function)
    graph.add_node("collect", collect_node)
    graph.add_node("plan", plan_node)
    graph.add_node("normalize", normalize_node)
    graph.add_node("analyze", analyze_node)
    graph.add_node("summarize", summarize_node)
    graph.add_node("critic", critic_node)
    graph.add_node("report", report_node)
    graph.add_node("notify", notify_node)

    # Set entry point (official pattern)
    graph.set_entry_point("collect")

    # Add edges for sequential pipeline (official pattern)
    graph.add_edge("collect", "plan")
    graph.add_edge("plan", "normalize")
    graph.add_edge("normalize", "analyze")
    graph.add_edge("analyze", "summarize")
    graph.add_edge("summarize", "critic")
    graph.add_edge("critic", "report")
    graph.add_edge("report", "notify")
    graph.add_edge("notify", END)

    # Compile graph (official pattern - required before execution)
    compiled_graph = graph.compile()

    _module_logger.info("LangGraph built and compiled successfully")

    return compiled_graph


def run_agent(query: str, time_window: str = "7d", language: str = "ko", max_results: int = 20) -> NewsAgentState:
    """
    ë‰´ìŠ¤ íŠ¸ë Œë“œ ì—ì´ì „íŠ¸ ì‹¤í–‰

    LangGraph ê³µì‹ íŒ¨í„´ì„ ë”°ë¥´ëŠ” ë©”ì¸ ì§„ì…ì ì…ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰ í‚¤ì›Œë“œ
        time_window: ì‹œê°„ ë²”ìœ„ (ì˜ˆ: "24h", "7d", "30d")
        language: ì–¸ì–´ ì½”ë“œ ("ko", "en")
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        ë¦¬í¬íŠ¸ì™€ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ìµœì¢… ìƒíƒœ
    """
    # Generate run_id
    run_id = str(uuid.uuid4())

    logger = AgentLogger("news_trend_agent", run_id)
    logger.info("Starting news trend agent", query=query, time_window=time_window, language=language)

    # Create initial state (official pattern: Pydantic model)
    initial_state = NewsAgentState(
        query=query,
        time_window=time_window,
        language=language,
        max_results=max_results,
        run_id=run_id
    )

    # Build and compile graph
    graph = build_graph()

    # Invoke graph (official pattern: invoke() for synchronous execution)
    try:
        final_state = graph.invoke(initial_state)
        logger.info("News trend agent completed successfully", run_id=run_id)
    except Exception as e:
        logger.error("News trend agent failed", error=str(e), run_id=run_id)
        raise

    return final_state
