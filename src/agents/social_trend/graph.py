from __future__ import annotations

import os
import json
import uuid
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

from langgraph.graph import StateGraph, END

from src.core.state import SocialTrendAgentState
from src.core.logging import AgentLogger
from src.core.errors import safe_api_call
from src.core.config import get_config_manager
from src.agents.social_trend.tools import (
    fetch_x_posts,
    fetch_instagram_posts,
    fetch_naver_blog_posts,
    fetch_rss_feeds,
    normalize_items,
    analyze_sentiment_and_keywords,
)
from src.integrations.llm import get_llm_client
from src.integrations.retrieval.vectorstore_pinecone import PineconeVectorStore

logger = logging.getLogger(__name__)

ARTIFACTS_DIR = Path(__file__).resolve().parents[2] / "artifacts" / "social_trend_agent"
ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)


def _get_llm_client():
    """Social Trend ì—ì´ì „íŠ¸ ì „ìš© LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        return get_llm_client(agent_name="social_trend_agent")
    except Exception as e:
        logger.warning(f"Failed to initialize LLM client: {e}")
        return None


def _get_vector_store():
    """Social Trend ì—ì´ì „íŠ¸ ì „ìš© ë²¡í„° ìŠ¤í† ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        cfg = get_config_manager()
        agent_cfg = cfg.get_agent_config("social_trend_agent")
        vs_cfg = agent_cfg.vector_store if agent_cfg and agent_cfg.vector_store else {}
        index_name = vs_cfg.get("index_name", "social-trend-index")
        return PineconeVectorStore(index_name=index_name)
    except Exception as e:
        logger.warning(f"Failed to initialize vector store: {e}")
        return None


def _generate_llm_insights(
    query: str,
    normalized: List[Dict[str, Any]],
    analysis: Dict[str, Any],
    sources: List[str],
    time_window: str
) -> str:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹¬ì¸µ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    llm_client = _get_llm_client()
    if not llm_client:
        return "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    try:
        from src.agents.social_trend.prompts import SUMMARIZE_PROMPT_TEMPLATE

        # Prepare data for LLM
        sentiment = analysis.get("sentiment", {})
        keywords_data = analysis.get("keywords", {})
        top_keywords = keywords_data.get("top_keywords", [])

        keywords_str = "\n".join([
            f"- {kw['keyword']}: {kw['count']}íšŒ ì–¸ê¸‰"
            for kw in top_keywords[:10]
        ])

        social_items_str = "\n\n".join([
            f"[{item.get('source', 'Unknown')}] {item.get('title', '')}\n{item.get('content', '')[:200]}..."
            for item in normalized[:15]
        ])

        prompt = SUMMARIZE_PROMPT_TEMPLATE.format(
            query=query,
            time_window=time_window,
            sources=", ".join(sources),
            item_count=len(normalized),
            positive=sentiment.get("positive_pct", 0),
            neutral=sentiment.get("neutral_pct", 0),
            negative=sentiment.get("negative_pct", 0),
            keywords=keywords_str,
            social_items=social_items_str
        )

        return llm_client.invoke(prompt)

    except Exception as e:
        logger.warning(f"LLM insight generation failed: {e}")
        return f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# =============================================================================
# LangGraph Node Functions
# =============================================================================

def collect_node(state: SocialTrendAgentState) -> Dict[str, Any]:
    """ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„° ìˆ˜ì§‘ ë…¸ë“œ"""
    agent_logger = AgentLogger("social_trend_agent", state.run_id)
    agent_logger.node_start("collect")

    all_items = []
    max_per_platform = state.max_results_per_platform // len(state.platforms) if state.platforms else 10

    for platform in state.platforms:
        try:
            if platform == "x":
                items = safe_api_call(
                    fetch_x_posts,
                    state.query,
                    max_results=max_per_platform,
                    default=[]
                )
            elif platform == "instagram":
                items = safe_api_call(
                    fetch_instagram_posts,
                    state.query,
                    max_results=max_per_platform,
                    default=[]
                )
            elif platform == "naver_blog":
                items = safe_api_call(
                    fetch_naver_blog_posts,
                    state.query,
                    max_results=max_per_platform,
                    default=[]
                )
            else:
                items = []

            if items:
                all_items.extend(items)
                logger.info(f"Collected {len(items)} items from {platform}")
            else:
                logger.info(f"No items collected from {platform}")

        except Exception as e:
            logger.error(f"Error collecting from {platform}: {e}")

    # RSS feeds
    if state.include_rss:
        feeds = state.rss_feeds or [
            "https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml",
            "https://www.reddit.com/r/MachineLearning/.rss",
        ]
        rss_items = safe_api_call(
            fetch_rss_feeds,
            feeds,
            max_results=max_per_platform,
            default=[]
        )
        if rss_items:
            all_items.extend(rss_items)

    agent_logger.node_end("collect", {"items_count": len(all_items)})
    return {"raw_items": all_items}


def normalize_node(state: SocialTrendAgentState) -> Dict[str, Any]:
    """ë°ì´í„° ì •ê·œí™” ë…¸ë“œ"""
    agent_logger = AgentLogger("social_trend_agent", state.run_id)
    agent_logger.node_start("normalize")

    normalized = normalize_items(state.raw_items)

    agent_logger.node_end("normalize", {"normalized_count": len(normalized)})
    return {"normalized": normalized}


def analyze_node(state: SocialTrendAgentState) -> Dict[str, Any]:
    """ê°ì„± ë° í‚¤ì›Œë“œ ë¶„ì„ ë…¸ë“œ (Pinecone RAG ì§€ì›)"""
    agent_logger = AgentLogger("social_trend_agent", state.run_id)
    agent_logger.node_start("analyze")

    texts = [
        it.get("title", "") + "\n" + it.get("content", "")
        for it in state.normalized
    ]
    analysis = analyze_sentiment_and_keywords(texts)

    # Extract engagement stats per platform
    engagement_stats = {}
    for item in state.normalized:
        platform = item.get("source", "unknown")
        if platform not in engagement_stats:
            engagement_stats[platform] = {"count": 0, "total_engagement": 0}
        engagement_stats[platform]["count"] += 1

    # Index items in Pinecone for RAG
    try:
        llm_client = _get_llm_client()
        vector_store = _get_vector_store()

        if llm_client and vector_store and state.normalized:
            import hashlib
            # Build corpus
            ids = [hashlib.md5(t.encode()).hexdigest()[:12] for t in texts]
            vectors = llm_client.get_embeddings_batch(texts)

            # Prepare metadata
            metadatas = []
            for i, item in enumerate(state.normalized):
                meta = {
                    "index": i,
                    "title": item.get("title", "")[:500],
                    "source": item.get("source", ""),
                    "url": item.get("url", "")[:500]
                }
                metadatas.append(meta)

            # Upsert to Pinecone
            vector_store.upsert(ids, vectors, metadatas)
            logger.info(f"Indexed {len(ids)} items to Pinecone for social_trend_agent")

    except Exception as e:
        logger.warning(f"Failed to index items to Pinecone: {e}")

    agent_logger.node_end("analyze", {"sentiment": analysis.get("sentiment", {})})
    return {
        "analysis": analysis,
        "engagement_stats": engagement_stats
    }


def summarize_node(state: SocialTrendAgentState) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„± ë…¸ë“œ"""
    agent_logger = AgentLogger("social_trend_agent", state.run_id)
    agent_logger.node_start("summarize")

    llm_insights = _generate_llm_insights(
        query=state.query,
        normalized=state.normalized,
        analysis=state.analysis,
        sources=state.platforms,
        time_window=state.time_window or "7d"
    )

    summary = _make_summary(state.analysis)
    updated_analysis = {
        **state.analysis,
        "summary": summary,
        "llm_insights": llm_insights
    }

    agent_logger.node_end("summarize")
    return {"analysis": updated_analysis}


def report_node(state: SocialTrendAgentState) -> Dict[str, Any]:
    """ë¦¬í¬íŠ¸ ìƒì„± ë…¸ë“œ"""
    agent_logger = AgentLogger("social_trend_agent", state.run_id)
    agent_logger.node_start("report")

    metrics = _make_metrics(state.normalized, state.analysis)

    report_path = ARTIFACTS_DIR / f"{state.run_id}.md"
    _write_report(
        report_path,
        state.query,
        state.time_window or "7d",
        state.language,
        state.normalized,
        state.analysis,
        state.analysis.get("summary", ""),
        metrics,
        state.analysis.get("llm_insights", "")
    )

    agent_logger.node_end("report", {"report_path": str(report_path)})
    return {
        "metrics": metrics,
        "report_md": str(report_path)
    }


# =============================================================================
# Graph Builder
# =============================================================================

def build_graph() -> StateGraph:
    """Social Trend Agent ê·¸ë˜í”„ ë¹Œë“œ"""
    graph = StateGraph(SocialTrendAgentState)

    # Add nodes
    graph.add_node("collect", collect_node)
    graph.add_node("normalize", normalize_node)
    graph.add_node("analyze", analyze_node)
    graph.add_node("summarize", summarize_node)
    graph.add_node("report", report_node)

    # Add edges
    graph.set_entry_point("collect")
    graph.add_edge("collect", "normalize")
    graph.add_edge("normalize", "analyze")
    graph.add_edge("analyze", "summarize")
    graph.add_edge("summarize", "report")
    graph.add_edge("report", END)

    return graph


# =============================================================================
# Main Entry Point
# =============================================================================

def run_agent(
    query: str,
    sources: Optional[List[str]] = None,
    rss_feeds: Optional[List[str]] = None,
    time_window: str = "7d",
    language: str = "ko",
    max_results: int = 50,
) -> SocialTrendAgentState:
    """
    Social Trend Agent ì‹¤í–‰

    Args:
        query: ê²€ìƒ‰ì–´
        sources: ìˆ˜ì§‘í•  ì†ŒìŠ¤ í”Œë«í¼ ëª©ë¡
        rss_feeds: RSS í”¼ë“œ URL ëª©ë¡
        time_window: ì‹œê°„ ë²”ìœ„
        language: ì–¸ì–´ ì½”ë“œ
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        ìµœì¢… ìƒíƒœ
    """
    if sources is None:
        sources = ["x", "instagram", "naver_blog"]

    run_id = str(uuid.uuid4())[:8]

    # Initialize state
    initial_state = SocialTrendAgentState(
        query=query,
        time_window=time_window,
        language=language,
        platforms=sources,
        rss_feeds=rss_feeds or [],
        max_results_per_platform=max_results,
        include_rss=True,
        run_id=run_id
    )

    # Build and compile graph
    graph = build_graph()
    compiled = graph.compile()

    # Execute
    logger.info(f"Starting Social Trend Agent run: {run_id}")
    final_state = compiled.invoke(initial_state)

    logger.info(f"Completed Social Trend Agent run: {run_id}")
    return SocialTrendAgentState(**final_state)


# Legacy compatibility function
def run_agent_legacy(
    query: str,
    sources: Optional[List[str]] = None,
    rss_feeds: Optional[List[str]] = None,
    time_window: str = "7d",
    language: str = "ko",
    max_results: int = 50,
) -> Dict[str, Any]:
    """Legacy run_agent that returns Dict for backwards compatibility"""
    state = run_agent(query, sources, rss_feeds, time_window, language, max_results)
    return {
        "query": state.query,
        "time_window": state.time_window,
        "language": state.language,
        "normalized": state.normalized,
        "analysis": state.analysis,
        "metrics": state.metrics,
        "run_id": state.run_id,
        "report_md": state.report_md,
    }


def _make_summary(analysis: Dict[str, Any]) -> str:
    s = analysis.get("sentiment", {})
    k = analysis.get("keywords", {})
    top_kw = ", ".join([kw["keyword"] for kw in k.get("top_keywords", [])[:5]])
    return (
        f"ê¸ì • {s.get('positive_pct', 0):.1f}% / ì¤‘ë¦½ {s.get('neutral_pct', 0):.1f}% / "
        f"ë¶€ì • {s.get('negative_pct', 0):.1f}% | ì£¼ìš” í‚¤ì›Œë“œ: {top_kw}"
    )


def _make_metrics(normalized: List[Dict[str, Any]], analysis: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "coverage": min(1.0, len(normalized) / 100.0),
        "factuality": 0.7,  # placeholder
        "actionability": 0.6,  # placeholder
    }


def _write_report(
    path: Path,
    query: str,
    time_window: str,
    language: str,
    normalized: List[Dict[str, Any]],
    analysis: Dict[str, Any],
    summary: str,
    metrics: Dict[str, Any],
    llm_insights: str = ""
) -> None:
    lines = []
    lines.append(f"# Social Trend Report")
    lines.append("")
    lines.append(f"- **Query**: {query}")
    lines.append(f"- **Time Window**: {time_window}")
    lines.append(f"- **Language**: {language}")
    lines.append(f"- **Items Analyzed**: {len(normalized)}")
    lines.append("")

    lines.append("## ğŸ“Š Quick Summary")
    lines.append(summary)
    lines.append("")

    # Sentiment breakdown
    sentiment = analysis.get("sentiment", {})
    lines.append("## ğŸ’­ Sentiment Analysis")
    lines.append(f"- **Positive**: {sentiment.get('positive', 0)} ({sentiment.get('positive_pct', 0):.1f}%)")
    lines.append(f"- **Neutral**: {sentiment.get('neutral', 0)} ({sentiment.get('neutral_pct', 0):.1f}%)")
    lines.append(f"- **Negative**: {sentiment.get('negative', 0)} ({sentiment.get('negative_pct', 0):.1f}%)")
    lines.append("")

    # Keywords
    keywords_data = analysis.get("keywords", {})
    top_keywords = keywords_data.get("top_keywords", [])
    if top_keywords:
        lines.append("## ğŸ”‘ Top Keywords")
        for i, kw in enumerate(top_keywords[:10], 1):
            lines.append(f"{i}. **{kw['keyword']}** - {kw['count']} mentions")
        lines.append("")

    # LLM Insights
    if llm_insights and llm_insights != "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.":
        lines.append("## ğŸ’¡ AI-Generated Insights")
        lines.append(llm_insights)
        lines.append("")

    # Metrics
    lines.append("## ğŸ“ˆ Quality Metrics")
    lines.append(f"- **Coverage**: {metrics.get('coverage', 0):.2f}")
    lines.append(f"- **Factuality**: {metrics.get('factuality', 0):.2f}")
    lines.append(f"- **Actionability**: {metrics.get('actionability', 0):.2f}")
    lines.append("")

    # Top items
    lines.append("## ğŸ“± Top Social Posts")
    for i, it in enumerate(normalized[:10], 1):
        title = it.get('title', 'No title')
        url = it.get('url', '')
        source = it.get('source', 'Unknown')
        if url:
            lines.append(f"{i}. [{title}]({url}) - *{source}*")
        else:
            lines.append(f"{i}. {title} - *{source}*")
    lines.append("")

    path.write_text("\n".join(lines), encoding="utf-8")


