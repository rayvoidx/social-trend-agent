"""
Python Analysis Microservice

FastAPI ì„œë¹„ìŠ¤ë¡œ Pythonì˜ ê°•ì (ML, NLP, LLM)ì„ TypeScript API Gatewayì— ì œê³µ
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any
import logging

# Import from existing Python agent
from src.agents.news_trend.tools import (
    analyze_sentiment,
    extract_keywords,
    summarize_trend
)
from src.integrations.llm import get_llm_client

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI
app = FastAPI(
    title="Python Analysis Service",
    description="High-performance ML/NLP analysis service for hybrid architecture",
    version="1.0.0"
)

# CORS (TypeScript API Gatewayì—ì„œ í˜¸ì¶œ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜: íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =============================================================================
# Request/Response Models
# =============================================================================

class NewsItem(BaseModel):
    title: str
    description: str
    url: str
    source: str
    publishedAt: str
    content: str = ""


class SentimentRequest(BaseModel):
    items: List[Dict[str, Any]]


class KeywordRequest(BaseModel):
    items: List[Dict[str, Any]]


class SummarizeRequest(BaseModel):
    query: str
    normalized_items: List[Dict[str, Any]]
    analysis: Dict[str, Any]


# =============================================================================
# Health Check
# =============================================================================

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "python-analysis",
        "version": "1.0.0"
    }


# =============================================================================
# Analysis Endpoints
# =============================================================================

@app.post("/api/analyze/sentiment")
async def sentiment_analysis(request: SentimentRequest):
    """
    ê°ì„± ë¶„ì„
    
    Pythonì˜ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ê³ ê¸‰ ê°ì„± ë¶„ì„
    (í–¥í›„ Transformer ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ ê°€ëŠ¥)
    """
    try:
        logger.info(f"Sentiment analysis started: {len(request.items)} items")
        result = analyze_sentiment(request.items)
        logger.info(f"Sentiment analysis completed: {result}")
        return result
    except Exception as e:
        logger.error(f"Sentiment analysis failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/analyze/keywords")
async def keyword_extraction(request: KeywordRequest):
    """
    í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Pythonì˜ TF-IDF, Word2Vec ë“±ì„ í™œìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    try:
        logger.info(f"Keyword extraction started: {len(request.items)} items")
        result = extract_keywords(request.items)
        logger.info(f"Keyword extraction completed: {result.get('total_unique_keywords')} keywords")
        return result
    except Exception as e:
        logger.error(f"Keyword extraction failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/analyze/summarize")
async def summarization(request: SummarizeRequest):
    """
    LLM ê¸°ë°˜ íŠ¸ë Œë“œ ìš”ì•½
    
    LangChain Pythonì˜ ì„±ìˆ™í•œ ìƒíƒœê³„ë¥¼ í™œìš©í•œ ê³ ê¸‰ ìš”ì•½
    """
    try:
        logger.info(f"Summarization started: query={request.query}")
        result = summarize_trend(
            query=request.query,
            normalized_items=request.normalized_items,
            analysis=request.analysis
        )
        logger.info(f"Summarization completed: {len(result)} chars")
        return {"summary": result}
    except Exception as e:
        logger.error(f"Summarization failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Batch Analysis (ëŒ€ëŸ‰ ì²˜ë¦¬)
# =============================================================================

@app.post("/api/analyze/batch")
async def batch_analysis(request: SentimentRequest):
    """
    ë°°ì¹˜ ë¶„ì„ (ê°ì„± + í‚¤ì›Œë“œ + ìš”ì•½ í•œ ë²ˆì—)
    
    Pythonì—ì„œ ëª¨ë“  ë¶„ì„ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
    """
    try:
        logger.info(f"Batch analysis started: {len(request.items)} items")
        
        # ë³‘ë ¬ ì²˜ë¦¬ (Python asyncio)
        sentiment = analyze_sentiment(request.items)
        keywords = extract_keywords(request.items)
        
        result = {
            "sentiment": sentiment,
            "keywords": keywords,
            "total_items": len(request.items)
        }
        
        logger.info("Batch analysis completed")
        return result
    except Exception as e:
        logger.error(f"Batch analysis failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Advanced ML Endpoints (í–¥í›„ í™•ì¥)
# =============================================================================

class MLPredictRequest(BaseModel):
    task: str  # "sentiment", "trend", "anomaly", "classification"
    texts: List[str]
    options: Dict[str, Any] = {}


class MLTrainRequest(BaseModel):
    task: str
    training_data: List[Dict[str, Any]]
    model_config: Dict[str, Any] = {}
    async_mode: bool = True


@app.post("/api/ml/predict")
async def ml_prediction(request: MLPredictRequest):
    """
    ML ëª¨ë¸ ì˜ˆì¸¡

    LLM ê¸°ë°˜ ì˜ˆì¸¡ (íŠ¸ë Œë“œ ì˜ˆì¸¡, ë¶„ë¥˜, ì´ìƒ íƒì§€)
    """
    try:
        logger.info(f"ML prediction started: task={request.task}, items={len(request.texts)}")

        if request.task == "sentiment":
            return await _predict_sentiment(request.texts, request.options)
        elif request.task == "trend":
            return await _predict_trend(request.texts, request.options)
        elif request.task == "anomaly":
            return await _detect_anomaly(request.texts, request.options)
        elif request.task == "classification":
            return await _classify_texts(request.texts, request.options)
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Unknown task: {request.task}. Supported: sentiment, trend, anomaly, classification"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ML prediction failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


async def _predict_sentiment(texts: List[str], options: Dict[str, Any]) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ê°ì„± ì˜ˆì¸¡"""
    client = get_llm_client()

    prompt = f"""Analyze the sentiment of each text below and return a JSON array with predictions.

Texts:
{chr(10).join([f'{i+1}. {text[:500]}' for i, text in enumerate(texts[:20])])}

Return JSON format:
{{
    "predictions": [
        {{"text_index": 0, "sentiment": "positive"|"neutral"|"negative", "confidence": 0.0-1.0, "emotions": ["emotion1", "emotion2"]}}
    ],
    "summary": {{
        "positive_count": N,
        "neutral_count": N,
        "negative_count": N,
        "dominant_sentiment": "positive"|"neutral"|"negative"
    }}
}}"""

    result = client.chat_json(
        messages=[
            {"role": "system", "content": "You are a sentiment analysis expert. Return valid JSON only."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )

    return {
        "task": "sentiment",
        "predictions": result.get("predictions", []),
        "summary": result.get("summary", {}),
        "total_texts": len(texts)
    }


async def _predict_trend(texts: List[str], options: Dict[str, Any]) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ íŠ¸ë Œë“œ ì˜ˆì¸¡"""
    client = get_llm_client()

    time_horizon = options.get("time_horizon", "7d")

    prompt = f"""Analyze these texts and predict trends for the next {time_horizon}.

Texts:
{chr(10).join([f'- {text[:300]}' for text in texts[:15]])}

Return JSON format:
{{
    "predicted_trends": [
        {{"topic": "topic name", "direction": "rising"|"stable"|"declining", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}
    ],
    "emerging_topics": ["topic1", "topic2"],
    "declining_topics": ["topic1", "topic2"],
    "forecast_summary": "overall trend forecast"
}}"""

    result = client.chat_json(
        messages=[
            {"role": "system", "content": "You are a trend analysis expert. Return valid JSON only."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    return {
        "task": "trend",
        "time_horizon": time_horizon,
        "predicted_trends": result.get("predicted_trends", []),
        "emerging_topics": result.get("emerging_topics", []),
        "declining_topics": result.get("declining_topics", []),
        "forecast_summary": result.get("forecast_summary", "")
    }


async def _detect_anomaly(texts: List[str], options: Dict[str, Any]) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ì´ìƒ íƒì§€"""
    client = get_llm_client()

    prompt = f"""Analyze these texts and identify any anomalies, unusual patterns, or outliers.

Texts:
{chr(10).join([f'{i+1}. {text[:400]}' for i, text in enumerate(texts[:15])])}

Return JSON format:
{{
    "anomalies": [
        {{"text_index": 0, "anomaly_type": "type", "severity": "high"|"medium"|"low", "description": "why this is anomalous"}}
    ],
    "patterns": [
        {{"pattern": "description", "frequency": N, "significance": "high"|"medium"|"low"}}
    ],
    "overall_assessment": "summary of anomaly detection"
}}"""

    result = client.chat_json(
        messages=[
            {"role": "system", "content": "You are an anomaly detection expert. Return valid JSON only."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )

    return {
        "task": "anomaly",
        "anomalies": result.get("anomalies", []),
        "patterns": result.get("patterns", []),
        "overall_assessment": result.get("overall_assessment", ""),
        "total_texts": len(texts)
    }


async def _classify_texts(texts: List[str], options: Dict[str, Any]) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    client = get_llm_client()

    categories = options.get("categories", ["news", "opinion", "question", "announcement", "other"])

    prompt = f"""Classify each text into one of these categories: {', '.join(categories)}

Texts:
{chr(10).join([f'{i+1}. {text[:400]}' for i, text in enumerate(texts[:20])])}

Return JSON format:
{{
    "classifications": [
        {{"text_index": 0, "category": "category_name", "confidence": 0.0-1.0, "secondary_category": "optional"}}
    ],
    "category_distribution": {{
        "category_name": count
    }}
}}"""

    result = client.chat_json(
        messages=[
            {"role": "system", "content": "You are a text classification expert. Return valid JSON only."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )

    return {
        "task": "classification",
        "categories": categories,
        "classifications": result.get("classifications", []),
        "category_distribution": result.get("category_distribution", {}),
        "total_texts": len(texts)
    }


@app.post("/api/ml/train")
async def ml_training(request: MLTrainRequest):
    """
    ML ëª¨ë¸ í•™ìŠµ (Fine-tuning ë˜ëŠ” Few-shot Learning)

    Note: ì‹¤ì œ ëª¨ë¸ í•™ìŠµì€ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì²˜ë¦¬
    ì—¬ê¸°ì„œëŠ” Few-shot learningìš© ë°ì´í„° ì¤€ë¹„ë§Œ ìˆ˜í–‰
    """
    try:
        logger.info(f"ML training started: task={request.task}, samples={len(request.training_data)}")

        if len(request.training_data) < 3:
            raise HTTPException(
                status_code=400,
                detail="At least 3 training samples required"
            )

        # Validate training data
        validated_data = []
        for item in request.training_data:
            if "text" not in item or "label" not in item:
                raise HTTPException(
                    status_code=400,
                    detail="Each training sample must have 'text' and 'label' fields"
                )
            validated_data.append({
                "text": item["text"][:1000],
                "label": item["label"]
            })

        # Build few-shot prompt template
        few_shot_examples = "\n".join([
            f"Text: {item['text'][:200]}\nLabel: {item['label']}"
            for item in validated_data[:5]
        ])

        # Test the few-shot learning
        client = get_llm_client()
        test_prompt = f"""You are trained to classify text with these examples:

{few_shot_examples}

Now classify this test text:
Text: {validated_data[-1]['text'][:200]}
Label:"""

        test_result = client.invoke(test_prompt)

        return {
            "status": "completed",
            "task": request.task,
            "training_samples": len(validated_data),
            "model_type": "few-shot-learning",
            "few_shot_template": few_shot_examples,
            "test_prediction": test_result.strip(),
            "expected_label": validated_data[-1]["label"],
            "note": "Few-shot learning template created. Use this template for inference."
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ML training failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Startup/Shutdown Events
# =============================================================================

@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ Python Analysis Service started")
    logger.info("   Ready to serve ML/NLP requests from TypeScript API")


@app.on_event("shutdown")
async def shutdown_event():
    logger.info("ğŸ‘‹ Python Analysis Service shutting down")


# =============================================================================
# Run Server
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=9001,
        log_level="info"
    )

